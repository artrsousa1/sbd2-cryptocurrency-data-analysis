{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50816b36",
   "metadata": {},
   "source": [
    "# ETL: Silver to Gold\n",
    "\n",
    "Como já explicado anteriormente, o ETL é o processo de extração, transformação e carga de dados. Nesse notebook, vamos focar na transformação e carga dos dados do nível Silver para o nível Gold do nosso Data Warehouse. Dessa forma , estaremos preparando os dados para análises, possibilitando a utilização de ferramentas de BI para gerar os relatórios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0cf7c",
   "metadata": {},
   "source": [
    "## 1. Configuração inicial\n",
    "\n",
    "Essa célula importa todas as bibliotecas necessárias, define os caminhos para os arquivos de configuração (`.env`) e DDL (`gold_ddl.sql`), e cria a função `get_connection()` para se conectar ao PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b28dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuração inicial concluída.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extras import execute_values\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dotenv_path = Path('../../.env')\n",
    "ddl_file_path = Path('../../data_layer/gold/gold_ddl.sql')\n",
    "load_dotenv(dotenv_path=dotenv_path, override=True)\n",
    "\n",
    "def get_connection():\n",
    "    try:\n",
    "        return psycopg2.connect(\n",
    "            host=os.getenv('DB_HOST','localhost'),\n",
    "            database=os.getenv('POSTGRES_DB','postgres'),\n",
    "            user=os.getenv('POSTGRES_USER','postgres'),\n",
    "            password=os.getenv('POSTGRES_PASSWORD','postgres'),\n",
    "            port=os.getenv('DB_PORT', 5432)\n",
    "        )\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Falha ao conectar ao banco de dados: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"Configuração inicial concluída.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c47cf",
   "metadata": {},
   "source": [
    "## 2. Execução do DDL\n",
    "\n",
    "Essa célula garante que nosso *Data Warehouse* (o *schema* `dw`) exista e esteja no estado correto. Efetuamos a leitura do arquivo `gold_ddl.sql` e o executamos. Dentro do DDL, usamos comando como `DROP TABLE IF EXISTS ...` e `CREATE TABLE IF NOT EXISTS` para garantir que o notebook possa ser executado várias vezes sem erros.\n",
    "\n",
    "Algumas observações importantes sobre o DDL:\n",
    "\n",
    "1. **Modelo Star Schema:** A arquitetura é composta por uma tabela fato_metricas_crpt (contendo as métricas numéricas, como valor de capitalização de mercado) e três dimensões de contexto (dim_crpt, dim_data, dim_hora).\n",
    "\n",
    "2. **Chave Natural:** Ao analisar a camada silver, é possível observar que a coluna symbol estava corrompida (ex: 'USD' para todas as moedas), o que não condiz com a descrição das colunas no dataset. Dessa forma, optamos por utilizar a coluna name como chave natural. Assim, para um determinado snapshot (data e hora), cada criptomoeda é unicamente identificada pelo seu nome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e87344fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando DDL...\n",
      "Schema 'dw' e tabelas criadas com sucesso.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Executando DDL...\")\n",
    "try:\n",
    "    with open(ddl_file_path, 'r') as f:\n",
    "        ddl_script = f.read()\n",
    "    \n",
    "    with get_connection() as conn, conn.cursor() as cur:\n",
    "        cur.execute(ddl_script)\n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"Schema 'dw' e tabelas criadas com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao executar DDL: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60f9ef",
   "metadata": {},
   "source": [
    "## 3. Extract\n",
    "\n",
    "Na etapa de extração, lemos todos os dados do nosso Data Lakehouse da Camada Silver (`public.currencies_data`) e passamos para um DataFrame do Pandas (`df_silver`). Dessa forma, realizamos a extração completa dos dados, preparando-os para as etapas subsequentes com os dados em memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bb267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo dados da tabela 'public.currencies_data'...\n",
      "20225 linhas extraídas da Silver.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bk/sbn822613qn4469447xkk8fw0000gn/T/ipykernel_21793/609929424.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_silver = pd.read_sql_query(\"SELECT * FROM public.currencies_data\", conn)\n"
     ]
    }
   ],
   "source": [
    "print(\"Extraindo dados da tabela 'public.currencies_data'...\")\n",
    "df_silver = None\n",
    "try:\n",
    "    with get_connection() as conn:\n",
    "        df_silver = pd.read_sql_query(\"SELECT * FROM public.currencies_data\", conn)\n",
    "    \n",
    "    if df_silver is None or df_silver.empty:\n",
    "        raise Exception(\"A tabela Silver 'public.currencies_data' está vazia ou não foi carregada.\")\n",
    "    \n",
    "    print(f\"{len(df_silver)} linhas extraídas da Silver.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao extrair dados da Silver: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d518d",
   "metadata": {},
   "source": [
    "## 4. Transform\n",
    "\n",
    "Para iniciar a etapa de transformação, criamos três DataFrames do Panda que vão compor as nossas dimensões no Data Warehouse:\n",
    "\n",
    "1.  **`df_dim_data`**: Extrai as datas únicas de `last_updated` e cria atributos (ano, mês, trimestre).\n",
    "2.  **`df_dim_hora`**: Gera programaticamente os 86.400 segundos de um dia.\n",
    "3.  **`df_dim_crpt`**: Extrai as moedas únicas (baseado no `name`), trata os valores `Infinity` (convertendo para `NaN`) e renomeia as colunas.\n",
    "\n",
    "Depois, criamos o DataFrame da Fato (`df_fato`), que contém as métricas numéricas associadas a cada criptomoeda em um determinado snapshot (data e hora). Para isso, fazemos *merge* com as dimensões para obter os *surrogate keys* (SKs) correspondentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23134447",
   "metadata": {},
   "source": [
    "### 4.1 Transform das Dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27982cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformando dw.dim_dta...\n",
      "Transformando dw.dim_hra...\n",
      "Transformando dw.dim_crp...\n",
      "✅ Preparado: 1 datas, 86400 horas, 9193 criptos únicas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bk/sbn822613qn4469447xkk8fw0000gn/T/ipykernel_21793/257916200.py:19: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  times = pd.date_range('00:00:00', '23:59:59', freq='S').time\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformando dw.dim_dta...\")\n",
    "df_dim_dta = pd.DataFrame(pd.to_datetime(df_silver['last_updated']).dt.date.unique(), columns=['dte_cpt'])\n",
    "df_dim_dta = df_dim_dta.dropna()\n",
    "df_dim_dta['srk_dta'] = df_dim_dta['dte_cpt'].apply(lambda x: int(x.strftime('%Y%m%d')))\n",
    "df_dim_dta['num_dia'] = df_dim_dta['dte_cpt'].apply(lambda x: x.day)\n",
    "df_dim_dta['num_mes'] = df_dim_dta['dte_cpt'].apply(lambda x: x.month)\n",
    "df_dim_dta['num_ano'] = df_dim_dta['dte_cpt'].apply(lambda x: x.year)\n",
    "df_dim_dta['nom_mes'] = df_dim_dta['dte_cpt'].apply(lambda x: x.strftime('%B'))\n",
    "df_dim_dta['nom_sem'] = df_dim_dta['dte_cpt'].apply(lambda x: x.strftime('%A'))\n",
    "df_dim_dta['num_tri'] = (df_dim_dta['num_mes'] - 1) // 3 + 1\n",
    "df_dim_dta['num_sem'] = (df_dim_dta['num_mes'] - 1) // 6 + 1\n",
    "df_dim_dta['flg_fds'] = df_dim_dta['dte_cpt'].apply(lambda x: x.weekday() >= 5)\n",
    "\n",
    "df_dim_dta = df_dim_dta[['srk_dta', 'dte_cpt', 'num_dia', 'num_mes', 'num_ano', 'nom_mes', 'nom_sem', 'num_tri', 'num_sem', 'flg_fds']]\n",
    "\n",
    "print(\"Transformando dw.dim_hra...\")\n",
    "def get_periodo_dia(h):\n",
    "    return ('Manhã' if 6 <= h < 12 else 'Tarde' if 12 <= h < 18 else 'Noite' if 18 <= h < 24 else 'Madrugada')\n",
    "times = pd.date_range('00:00:00', '23:59:59', freq='S').time\n",
    "df_dim_hra = pd.DataFrame(times, columns=['hre_cpt'])\n",
    "df_dim_hra['srk_hra'] = df_dim_hra['hre_cpt'].apply(lambda x: x.hour * 10000 + x.minute * 100 + x.second)\n",
    "df_dim_hra['num_hra'] = [t.hour for t in times]\n",
    "df_dim_hra['num_min'] = [t.minute for t in times]\n",
    "df_dim_hra['num_seg'] = [t.second for t in times]\n",
    "df_dim_hra['nom_per'] = df_dim_hra['num_hra'].apply(get_periodo_dia)\n",
    "df_dim_hra = df_dim_hra[['srk_hra', 'hre_cpt', 'num_hra', 'num_min', 'num_seg', 'nom_per']]\n",
    "\n",
    "print(\"Transformando dw.dim_crp...\")\n",
    "cols = ['name', 'symbol', 'max_supply', 'is_active', 'date_added']\n",
    "df_dim_crp = df_silver[cols].drop_duplicates(subset=['name']).dropna(subset=['name']).copy()\n",
    "\n",
    "df_dim_crp.rename(columns={\n",
    "    'name': 'nky_nom', 'symbol': 'cod_sym', 'max_supply': 'vlr_max_sup',\n",
    "    'is_active': 'flg_atv', 'date_added': 'dte_add'\n",
    "}, inplace=True)\n",
    "\n",
    "df_dim_crp['vlr_max_sup'] = df_dim_crp['vlr_max_sup'].replace([np.inf, -np.inf], np.nan)\n",
    "df_dim_crp['dte_add'] = pd.to_datetime(df_dim_crp['dte_add']).dt.date\n",
    "df_dim_crp = df_dim_crp[['nky_nom', 'cod_sym', 'vlr_max_sup', 'flg_atv', 'dte_add']]\n",
    "\n",
    "print(f\"✅ Preparado: {len(df_dim_dta)} datas, {len(df_dim_hra)} horas, {len(df_dim_crp)} criptos únicas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0ab49",
   "metadata": {},
   "source": [
    "## 5. Load das Dimensões\n",
    "\n",
    "Para realizar a load das dimensões, carregamos os 3 DataFrames nas suas tabelas `dw.*`. Além disso, adicionamos um registro 'Desconhecido' (com SK = -1 ou 'N/A') em cada dimensão para garantir a integridade referencial, caso a Fato tenha uma chave que não foi encontrada.\n",
    "\n",
    "Armazenamos essas chaves 'desconhecidas' na variável `unknowns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e6a7fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dimensões no schema dw...\n",
      "✅ Dimensões carregadas. Unknowns: {'data': -1, 'hora': -1, 'crpt': 9194}\n"
     ]
    }
   ],
   "source": [
    "print(\"Carregando dimensões no schema dw...\")\n",
    "unknowns = {}\n",
    "try:\n",
    "    with get_connection() as conn, conn.cursor() as cur:\n",
    "        execute_values(cur, sql.SQL(\"\"\"\n",
    "            INSERT INTO dw.dim_dta (srk_dta, dte_cpt, num_dia, num_mes, num_ano, nom_mes, nom_sem, num_tri, num_sem, flg_fds)\n",
    "            VALUES %s ON CONFLICT (srk_dta) DO NOTHING\n",
    "        \"\"\"), df_dim_dta.values.tolist())\n",
    "\n",
    "        cur.execute(\"INSERT INTO dw.dim_dta (srk_dta, dte_cpt, num_dia, num_mes, num_ano) VALUES (-1, '1900-01-01', 1, 1, 1900) ON CONFLICT (srk_dta) DO NOTHING RETURNING srk_dta\")\n",
    "        unknowns['data'] = (cur.fetchone() or [-1])[0]\n",
    "\n",
    "        cur.execute(\"SELECT COUNT(*) FROM dw.dim_hra\")\n",
    "        if cur.fetchone()[0] < 86400:\n",
    "            execute_values(cur, sql.SQL(\"\"\"\n",
    "                INSERT INTO dw.dim_hra (srk_hra, hre_cpt, num_hra, num_min, num_seg, nom_per)\n",
    "                VALUES %s ON CONFLICT (srk_hra) DO NOTHING\n",
    "            \"\"\"), df_dim_hra.values.tolist(), page_size=5000)\n",
    "\n",
    "        cur.execute(\"INSERT INTO dw.dim_hra (srk_hra, hre_cpt, num_hra, num_min, num_seg) VALUES (-1, '00:00:00', 0, 0, 0) ON CONFLICT (srk_hra) DO NOTHING RETURNING srk_hra\")\n",
    "        unknowns['hora'] = (cur.fetchone() or [-1])[0]\n",
    "\n",
    "        execute_values(cur, sql.SQL(\"\"\"\n",
    "            INSERT INTO dw.dim_crp (nky_nom, cod_sym, vlr_max_sup, flg_atv, dte_add)\n",
    "            VALUES %s ON CONFLICT (nky_nom) DO UPDATE SET\n",
    "                cod_sym = EXCLUDED.cod_sym,\n",
    "                vlr_max_sup = EXCLUDED.vlr_max_sup,\n",
    "                flg_atv = EXCLUDED.flg_atv,\n",
    "                dte_add = EXCLUDED.dte_add\n",
    "        \"\"\"), df_dim_crp.where(pd.notna(df_dim_crp), None).values.tolist())\n",
    "\n",
    "        cur.execute(\"INSERT INTO dw.dim_crp (nky_nom) VALUES ('N/A') ON CONFLICT (nky_nom) DO NOTHING RETURNING srk_crp\")\n",
    "        res_crp = cur.fetchone()\n",
    "        unknowns['crpt'] = res_crp[0] if res_crp else cur.execute(\"SELECT srk_crp FROM dw.dim_crp WHERE nky_nom = 'N/A'\") or cur.fetchone()[0]\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    print(f\"✅ Dimensões carregadas. Unknowns: {unknowns}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao carregar dimensões: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed128e",
   "metadata": {},
   "source": [
    "## 6. Transform e Load da Tabela Fato\n",
    "\n",
    "Agora, já com as dimensões carregadas no Data Warehouse, podemos transformar e carregar a tabela `fato_metricas_crpt`. Para isso, seguimos os seguintes passos:\n",
    "\n",
    "1. Lemos as surrogate keys das dimensões do Data Warehouse para DataFrames.\n",
    "2. Fazemos *merge* entre o DataFrame da Fato e os DataFrames das dimensões para obter as SKs.\n",
    "3. Renomeamos as colunas da Silver para padronizar com o que foi definido na camada Gold. Além disso tratamos valores `Infinity` e `NaN`, e selecionamos a ordem final das colunas.\n",
    "4. Executamos `TRUNCATE` na Fato para garantir que quaiquer dados antigos sejam removidos. Em seguida, usamos `execute_values` para carregar o DataFrame direto na tabela `dw.fato_metricas_crpt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1484e30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando dw.fat_mtr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bk/sbn822613qn4469447xkk8fw0000gn/T/ipykernel_21793/1444108694.py:4: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_dta_lkp = pd.read_sql(\"SELECT srk_dta, dte_cpt FROM dw.dim_dta\", conn)\n",
      "/var/folders/bk/sbn822613qn4469447xkk8fw0000gn/T/ipykernel_21793/1444108694.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_hra_lkp = pd.read_sql(\"SELECT srk_hra, hre_cpt FROM dw.dim_hra\", conn)\n",
      "/var/folders/bk/sbn822613qn4469447xkk8fw0000gn/T/ipykernel_21793/1444108694.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_crp_lkp = pd.read_sql(\"SELECT srk_crp, nky_nom FROM dw.dim_crp\", conn)\n",
      "/var/folders/bk/sbn822613qn4469447xkk8fw0000gn/T/ipykernel_21793/1444108694.py:10: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  df_fato['join_time'] = pd.to_datetime(df_fato['last_updated']).dt.floor('S').dt.time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpando Fato (TRUNCATE)...\n",
      "Inserindo 20225 registros na Fato...\n",
      "✅ Fato carregada com sucesso: 20225 registros.\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparando dw.fat_mtr...\")\n",
    "try:\n",
    "    with get_connection() as conn:\n",
    "        df_dta_lkp = pd.read_sql(\"SELECT srk_dta, dte_cpt FROM dw.dim_dta\", conn)\n",
    "        df_hra_lkp = pd.read_sql(\"SELECT srk_hra, hre_cpt FROM dw.dim_hra\", conn)\n",
    "        df_crp_lkp = pd.read_sql(\"SELECT srk_crp, nky_nom FROM dw.dim_crp\", conn)\n",
    "\n",
    "    df_fato = df_silver.copy()\n",
    "    df_fato['join_date'] = pd.to_datetime(df_fato['last_updated']).dt.date\n",
    "    df_fato['join_time'] = pd.to_datetime(df_fato['last_updated']).dt.floor('S').dt.time\n",
    "    \n",
    "    df_dta_lkp['dte_cpt'] = pd.to_datetime(df_dta_lkp['dte_cpt']).dt.date\n",
    "    df_hra_lkp['hre_cpt'] = pd.to_datetime(df_hra_lkp['hre_cpt'], format='%H:%M:%S').dt.time\n",
    "\n",
    "    df_fato = df_fato.merge(df_crp_lkp, left_on='name', right_on='nky_nom', how='left')\\\n",
    "                     .merge(df_dta_lkp, left_on='join_date', right_on='dte_cpt', how='left')\\\n",
    "                     .merge(df_hra_lkp, left_on='join_time', right_on='hre_cpt', how='left')\n",
    "\n",
    "    df_fato['srk_crp'] = df_fato['srk_crp'].fillna(unknowns['crpt']).astype(int)\n",
    "    df_fato['srk_dta'] = df_fato['srk_dta'].fillna(unknowns['data']).astype(int)\n",
    "    df_fato['srk_hra'] = df_fato['srk_hra'].fillna(unknowns['hora']).astype(int)\n",
    "\n",
    "    rename_cols = {\n",
    "        'cmc_rank': 'rnk_cmc', 'price': 'vlr_pre_usd', 'volume_24h': 'vlr_vlm_24h',\n",
    "        'market_cap': 'vlr_mkt', 'dominance': 'vlr_dom', 'market_pair_count': 'qtd_par',\n",
    "        'circulating_supply': 'qtd_cir_sup', 'total_supply': 'qtd_tot_sup',\n",
    "        'fully_dillutted_market_cap': 'vlr_fld_mkt', 'market_cap_by_total_supply': 'vlr_mkt_tot',\n",
    "        'ytd_price_change_percentage': 'pct_ytd', 'percent_change_1h': 'pct_1hr', 'percent_change_24h': 'pct_24h',\n",
    "        'percent_change_7d': 'pct_7dd', 'percent_change_30d': 'pct_30d', 'percent_change_60d': 'pct_60d', 'percent_change_90d': 'pct_90d'\n",
    "    }\n",
    "    df_fato.rename(columns=rename_cols, inplace=True)\n",
    "    df_fato['vlr_tvr'] = np.nan\n",
    "    \n",
    "    cols_final = [\n",
    "        'srk_crp', 'srk_dta', 'srk_hra',\n",
    "        'rnk_cmc', 'vlr_pre_usd', 'vlr_vlm_24h', 'vlr_mkt', 'vlr_dom', 'vlr_tvr',\n",
    "        'qtd_par', 'qtd_cir_sup', 'qtd_tot_sup', 'vlr_fld_mkt', 'vlr_mkt_tot',\n",
    "        'pct_ytd', 'pct_1hr', 'pct_24h', 'pct_7dd', 'pct_30d', 'pct_60d', 'pct_90d'\n",
    "    ]\n",
    "    \n",
    "    df_fato_final = df_fato[cols_final].replace([np.inf, -np.inf], np.nan)\n",
    "    df_fato_list = df_fato_final.where(pd.notna(df_fato_final), None).values.tolist()\n",
    "\n",
    "    with get_connection() as conn, conn.cursor() as cur:\n",
    "        print(\"Limpando Fato (TRUNCATE)...\")\n",
    "        cur.execute(\"TRUNCATE TABLE dw.fat_mtr RESTART IDENTITY;\")\n",
    "        \n",
    "        print(f\"Inserindo {len(df_fato_list)} registros na Fato...\")\n",
    "        query = sql.SQL(\"INSERT INTO dw.fat_mtr ({}) VALUES %s\").format(sql.SQL(', ').join(map(sql.Identifier, cols_final)))\n",
    "        execute_values(cur, query, df_fato_list, page_size=1000)\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"✅ Fato carregada com sucesso: {len(df_fato_list)} registros.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao carregar fato: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6608753",
   "metadata": {},
   "source": [
    "## Validação final\n",
    "\n",
    "Esta seção executa uma contagem de linhas em todas as tabelas para verificar o resultado final. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f05bcdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bk/sbn822613qn4469447xkk8fw0000gn/T/ipykernel_21793/2855465976.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  display(pd.read_sql_query(query, conn))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tabela</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dw.dim_crp</td>\n",
       "      <td>9194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dw.dim_dta</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dw.dim_hra</td>\n",
       "      <td>86401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dw.fat_mtr</td>\n",
       "      <td>20225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tabela  total\n",
       "0  dw.dim_crp   9194\n",
       "1  dw.dim_dta      2\n",
       "2  dw.dim_hra  86401\n",
       "3  dw.fat_mtr  20225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    with get_connection() as conn:\n",
    "        query = \"\"\"\n",
    "        SELECT 'dw.dim_crp' AS Tabela, COUNT(*) AS Total FROM dw.dim_crp\n",
    "        UNION ALL SELECT 'dw.dim_dta', COUNT(*) FROM dw.dim_dta\n",
    "        UNION ALL SELECT 'dw.dim_hra', COUNT(*) FROM dw.dim_hra\n",
    "        UNION ALL SELECT 'dw.fat_mtr', COUNT(*) FROM dw.fat_mtr;\n",
    "        \"\"\"\n",
    "        display(pd.read_sql_query(query, conn))\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
