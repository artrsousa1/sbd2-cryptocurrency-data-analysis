{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50816b36",
   "metadata": {},
   "source": [
    "# ETL: Silver to Gold\n",
    "\n",
    "Como já explicado anteriormente, o ETL é o processo de extração, transformação e carga de dados. Nesse notebook, vamos focar na transformação e carga dos dados do nível Silver para o nível Gold do nosso Data Warehouse. Dessa forma , estaremos preparando os dados para análises, possibilitando a utilização de ferramentas de BI para gerar os relatórios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0cf7c",
   "metadata": {},
   "source": [
    "## 1. Configuração inicial\n",
    "\n",
    "Essa célula importa todas as bibliotecas necessárias, define os caminhos para os arquivos de configuração (`.env`) e DDL (`gold_ddl.sql`), e cria a função `get_connection()` para se conectar ao PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b28dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extras import execute_values\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dotenv_path = Path('../../.env')\n",
    "ddl_file_path = Path('../../data_layer/gold/gold_ddl.sql')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "def get_connection():\n",
    "    try:\n",
    "        return psycopg2.connect(\n",
    "            host=os.getenv('DB_HOST','localhost'),\n",
    "            database=os.getenv('POSTGRES_DB','postgres'),\n",
    "            user=os.getenv('POSTGRES_USER','postgres'),\n",
    "            password=os.getenv('POSTGRES_PASSWORD','postgres'),\n",
    "            port=os.getenv('DB_PORT', 5432)\n",
    "        )\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Falha ao conectar ao banco de dados: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"Configuração inicial concluída.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c47cf",
   "metadata": {},
   "source": [
    "## 2. Execução do DDL\n",
    "\n",
    "Essa célula garante que nosso *Data Warehouse* (o *schema* `dw`) exista e esteja no estado correto. Efetuamos a leitura do arquivo `gold_ddl.sql` e o executamos. Dentro do DDL, usamos comando como `DROP TABLE IF EXISTS ...` e `CREATE TABLE IF NOT EXISTS` para garantir que o notebook possa ser executado várias vezes sem erros.\n",
    "\n",
    "Algumas observações importantes sobre o DDL:\n",
    "\n",
    "1. **Modelo Star Schema:** A arquitetura é composta por uma tabela fato_metricas_crpt (contendo as métricas numéricas, como valor de capitalização de mercado) e três dimensões de contexto (dim_crpt, dim_data, dim_hora).\n",
    "\n",
    "2. **Chave Natural:** Ao analisar a camada silver, é possível observar que a coluna symbol estava corrompida (ex: 'USD' para todas as moedas), o que não condiz com a descrição das colunas no dataset. Dessa forma, optamos por utilizar a coluna name como chave natural. Assim, para um determinado snapshot (data e hora), cada criptomoeda é unicamente identificada pelo seu nome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87344fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Executando DDL...\")\n",
    "try:\n",
    "    with open(ddl_file_path, 'r') as f:\n",
    "        ddl_script = f.read()\n",
    "    \n",
    "    with get_connection() as conn, conn.cursor() as cur:\n",
    "        cur.execute(ddl_script)\n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"Schema 'dw' e tabelas criadas com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao executar DDL: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60f9ef",
   "metadata": {},
   "source": [
    "## 3. Extract\n",
    "\n",
    "Na etapa de extração, lemos todos os dados do nosso Data Lakehouse da Camada Silver (`public.currencies_data`) e passamos para um DataFrame do Pandas (`df_silver`). Dessa forma, realizamos a extração completa dos dados, preparando-os para as etapas subsequentes com os dados em memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extraindo dados da tabela 'public.currencies_data'...\")\n",
    "df_silver = None\n",
    "try:\n",
    "    with get_connection() as conn:\n",
    "        df_silver = pd.read_sql_query(\"SELECT * FROM public.currencies_data\", conn)\n",
    "    \n",
    "    if df_silver is None or df_silver.empty:\n",
    "        raise Exception(\"A tabela Silver 'public.currencies_data' está vazia ou não foi carregada.\")\n",
    "    \n",
    "    print(f\"{len(df_silver)} linhas extraídas da Silver.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao extrair dados da Silver: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d518d",
   "metadata": {},
   "source": [
    "## 4. Transform\n",
    "\n",
    "Para iniciar a etapa de transformação, criamos três DataFrames do Panda que vão compor as nossas dimensões no Data Warehouse:\n",
    "\n",
    "1.  **`df_dim_data`**: Extrai as datas únicas de `last_updated` e cria atributos (ano, mês, trimestre).\n",
    "2.  **`df_dim_hora`**: Gera programaticamente os 86.400 segundos de um dia.\n",
    "3.  **`df_dim_crpt`**: Extrai as moedas únicas (baseado no `name`), trata os valores `Infinity` (convertendo para `NaN`) e renomeia as colunas.\n",
    "\n",
    "Depois, criamos o DataFrame da Fato (`df_fato`), que contém as métricas numéricas associadas a cada criptomoeda em um determinado snapshot (data e hora). Para isso, fazemos *merge* com as dimensões para obter os *surrogate keys* (SKs) correspondentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23134447",
   "metadata": {},
   "source": [
    "### 4.1 Transform das Dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27982cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformando dw.dim_data...\")\n",
    "df_dim_data = pd.DataFrame(pd.to_datetime(df_silver['last_updated']).dt.date.unique(), columns=['dt_cpta'])\n",
    "df_dim_data = df_dim_data.dropna()\n",
    "df_dim_data['sk_data'] = df_dim_data['dt_cpta'].apply(lambda x: int(x.strftime('%Y%m%d')))\n",
    "df_dim_data['nr_dia'] = df_dim_data['dt_cpta'].apply(lambda x: x.day)\n",
    "df_dim_data['nr_mes'] = df_dim_data['dt_cpta'].apply(lambda x: x.month)\n",
    "df_dim_data['nr_ano'] = df_dim_data['dt_cpta'].apply(lambda x: x.year)\n",
    "df_dim_data['nm_mes'] = df_dim_data['dt_cpta'].apply(lambda x: x.strftime('%B'))\n",
    "df_dim_data['nm_d_sem'] = df_dim_data['dt_cpta'].apply(lambda x: x.strftime('%A'))\n",
    "df_dim_data['nr_trim'] = (df_dim_data['nr_mes'] - 1) // 3 + 1\n",
    "df_dim_data['nr_sem'] = (df_dim_data['nr_mes'] - 1) // 6 + 1\n",
    "df_dim_data['fl_fds'] = df_dim_data['dt_cpta'].apply(lambda x: x.weekday() >= 5)\n",
    "\n",
    "df_dim_data = df_dim_data[['sk_data', 'dt_cpta', 'nr_dia', 'nr_mes', 'nr_ano', 'nm_mes', 'nm_d_sem', 'nr_trim', 'nr_sem', 'fl_fds']]\n",
    "\n",
    "print(\"Transformando dw.dim_hora...\")\n",
    "def get_periodo_dia(h):\n",
    "    return ('Manhã' if 6 <= h < 12 else 'Tarde' if 12 <= h < 18 else 'Noite' if 18 <= h < 24 else 'Madrugada')\n",
    "times = pd.date_range('00:00:00', '23:59:59', freq='S').time\n",
    "df_dim_hora = pd.DataFrame(times, columns=['hr_cpta'])\n",
    "df_dim_hora['sk_hora'] = df_dim_hora['hr_cpta'].apply(lambda x: x.hour * 10000 + x.minute * 100 + x.second)\n",
    "df_dim_hora['nr_hora'] = [t.hour for t in times]\n",
    "df_dim_hora['nr_min'] = [t.minute for t in times]\n",
    "df_dim_hora['nr_seg'] = [t.second for t in times]\n",
    "df_dim_hora['nm_perdd'] = df_dim_hora['nr_hora'].apply(get_periodo_dia)\n",
    "df_dim_hora = df_dim_hora[['sk_hora', 'hr_cpta', 'nr_hora', 'nr_min', 'nr_seg', 'nm_perdd']]\n",
    "\n",
    "print(\"Transformando dw.dim_crpt...\")\n",
    "cols = ['name', 'symbol', 'max_supply', 'is_active', 'date_added']\n",
    "df_dim_crpt = df_silver[cols].drop_duplicates(subset=['name']).dropna(subset=['name']).copy()\n",
    "df_dim_crpt.rename(columns={\n",
    "    'name': 'nk_nome', 'symbol': 'cd_symbol', 'max_supply': 'vlr_max_supply',\n",
    "    'is_active': 'fl_ativa', 'date_added': 'dt_add'\n",
    "}, inplace=True)\n",
    "df_dim_crpt['vlr_max_supply'] = df_dim_crpt['vlr_max_supply'].replace([np.inf, -np.inf], np.nan)\n",
    "df_dim_crpt['dt_add'] = pd.to_datetime(df_dim_crpt['dt_add']).dt.date\n",
    "df_dim_crpt = df_dim_crpt[['nk_nome', 'cd_symbol', 'vlr_max_supply', 'fl_ativa', 'dt_add']]\n",
    "\n",
    "print(f\"{len(df_dim_data)} datas, {len(df_dim_hora)} horas, {len(df_dim_crpt)} criptos únicas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0ab49",
   "metadata": {},
   "source": [
    "## 5. Load das Dimensões\n",
    "\n",
    "Para realizar a load das dimensões, carregamos os 3 DataFrames nas suas tabelas `dw.*`. Além disso, adicionamos um registro 'Desconhecido' (com SK = -1 ou 'N/A') em cada dimensão para garantir a integridade referencial, caso a Fato tenha uma chave que não foi encontrada.\n",
    "\n",
    "Armazenamos essas chaves 'desconhecidas' na variável `unknowns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Carregando dimensões no schema dw...\")\n",
    "unknowns = {}\n",
    "try:\n",
    "    with get_connection() as conn, conn.cursor() as cur:\n",
    "        execute_values(cur, sql.SQL(\"\"\"\n",
    "            INSERT INTO dw.dim_data (sk_data, dt_cpta, nr_dia, nr_mes, nr_ano, nm_mes, nm_d_sem, nr_trim, nr_sem, fl_fds)\n",
    "            VALUES %s ON CONFLICT (sk_data) DO NOTHING\n",
    "        \"\"\"), df_dim_data.values.tolist())\n",
    "        cur.execute(\"INSERT INTO dw.dim_data (sk_data, dt_cpta) VALUES (-1, '1900-01-01') ON CONFLICT (sk_data) DO NOTHING RETURNING sk_data\")\n",
    "        unknowns['data'] = (cur.fetchone() or [-1])[0]\n",
    "\n",
    "        cur.execute(\"SELECT COUNT(*) FROM dw.dim_hora\")\n",
    "        if cur.fetchone()[0] < 86400:\n",
    "            execute_values(cur, sql.SQL(\"\"\"\n",
    "                INSERT INTO dw.dim_hora (sk_hora, hr_cpta, nr_hora, nr_min, nr_seg, nm_perdd)\n",
    "                VALUES %s ON CONFLICT (sk_hora) DO NOTHING\n",
    "            \"\"\"), df_dim_hora.values.tolist(), page_size=5000)\n",
    "        cur.execute(\"INSERT INTO dw.dim_hora (sk_hora, hr_cpta) VALUES (-1, '00:00:00') ON CONFLICT (sk_hora) DO NOTHING RETURNING sk_hora\")\n",
    "        unknowns['hora'] = (cur.fetchone() or [-1])[0]\n",
    "\n",
    "        print(\"Carregando dw.dim_crpt...\")\n",
    "        execute_values(cur, sql.SQL(\"\"\"\n",
    "            INSERT INTO dw.dim_crpt (nk_nome, cd_symbol, vlr_max_supply, fl_ativa, dt_add)\n",
    "            VALUES %s ON CONFLICT (nk_nome) DO UPDATE SET\n",
    "                cd_symbol = EXCLUDED.cd_symbol,\n",
    "                vlr_max_supply = EXCLUDED.vlr_max_supply,\n",
    "                fl_ativa = EXCLUDED.fl_ativa,\n",
    "                dt_add = EXCLUDED.dt_add\n",
    "        \"\"\"), df_dim_crpt.where(pd.notna(df_dim_crpt), None).values.tolist())\n",
    "        \n",
    "        cur.execute(\"INSERT INTO dw.dim_crpt (nk_nome) VALUES ('N/A') ON CONFLICT (nk_nome) DO NOTHING RETURNING sk_crpt\")\n",
    "        unknown_crpt_key_result = cur.fetchone()\n",
    "        if unknown_crpt_key_result:\n",
    "            unknowns['crpt'] = unknown_crpt_key_result[0]\n",
    "        else:\n",
    "            cur.execute(\"SELECT sk_crpt FROM dw.dim_crpt WHERE nk_nome = 'N/A'\")\n",
    "            unknowns['crpt'] = cur.fetchone()[0]\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    print(f\"Dimensões carregadas. Unknowns: {unknowns}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dimensões: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed128e",
   "metadata": {},
   "source": [
    "## 6. Transform e Load da Tabela Fato\n",
    "\n",
    "Agora, já com as dimensões carregadas no Data Warehouse, podemos transformar e carregar a tabela `fato_metricas_crpt`. Para isso, seguimos os seguintes passos:\n",
    "\n",
    "1. Lemos as surrogate keys das dimensões do Data Warehouse para DataFrames.\n",
    "2. Fazemos *merge* entre o DataFrame da Fato e os DataFrames das dimensões para obter as SKs.\n",
    "3. Renomeamos as colunas da Silver para padronizar com o que foi definido na camada Gold. Além disso tratamos valores `Infinity` e `NaN`, e selecionamos a ordem final das colunas.\n",
    "4. Executamos `TRUNCATE` na Fato para garantir que quaiquer dados antigos sejam removidos. Em seguida, usamos `execute_values` para carregar o DataFrame direto na tabela `dw.fato_metricas_crpt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando Transformação e Carga da Tabela Fato...\")\n",
    "try:\n",
    "    with get_connection() as conn:\n",
    "        df_data_lkp = pd.read_sql(\"SELECT sk_data, dt_cpta FROM dw.dim_data\", conn)\n",
    "        df_hora_lkp = pd.read_sql(\"SELECT sk_hora, hr_cpta FROM dw.dim_hora\", conn)\n",
    "        df_crpt_lkp = pd.read_sql(\"SELECT sk_crpt, nk_nome FROM dw.dim_crpt\", conn)\n",
    "\n",
    "    df_fato = df_silver.copy()\n",
    "    df_fato['join_date'] = pd.to_datetime(df_fato['last_updated']).dt.date\n",
    "    df_fato['join_time'] = pd.to_datetime(df_fato['last_updated']).dt.floor('S').dt.time\n",
    "    df_data_lkp['dt_cpta'] = pd.to_datetime(df_data_lkp['dt_cpta']).dt.date\n",
    "    df_hora_lkp['hr_cpta'] = pd.to_datetime(df_hora_lkp['hr_cpta'], format='%H:%M:%S').dt.time\n",
    "\n",
    "    df_fato = df_fato.merge(df_crpt_lkp, left_on='name', right_on='nk_nome', how='left')\\\n",
    "                     .merge(df_data_lkp, left_on='join_date', right_on='dt_cpta', how='left')\\\n",
    "                     .merge(df_hora_lkp, left_on='join_time', right_on='hr_cpta', how='left')\n",
    "\n",
    "    df_fato['sk_crpt'] = df_fato['sk_crpt'].fillna(unknowns['crpt']).astype(int)\n",
    "    df_fato['sk_data'] = df_fato['sk_data'].fillna(unknowns['data']).astype(int)\n",
    "    df_fato['sk_hora'] = df_fato['sk_hora'].fillna(unknowns['hora']).astype(int)\n",
    "\n",
    "    rename_cols = {\n",
    "        'cmc_rank': 'rnk_cmc', 'price': 'vlr_preco_usd', 'volume_24h': 'vlr_volume_24h',\n",
    "        'market_cap': 'vlr_mktcap', 'dominance': 'vlr_dmn', 'market_pair_count': 'qtd_pairs',\n",
    "        'circulating_supply': 'qtd_circ_sup', 'total_supply': 'qtd_tot_sup',\n",
    "        'fully_dillutted_market_cap': 'vlr_fd_mktcap', 'market_cap_by_total_supply': 'vlr_mcap_ts',\n",
    "        'ytd_price_change_percentage': 'pc_ytd', 'percent_change_1h': 'pc_1h', 'percent_change_24h': 'pc_24h',\n",
    "        'percent_change_7d': 'pc_7d', 'percent_change_30d': 'pc_30d', 'percent_change_60d': 'pc_60d', 'percent_change_90d': 'pc_90d'\n",
    "    }\n",
    "\n",
    "    df_fato.rename(columns=rename_cols, inplace=True)\n",
    "    df_fato['vlr_tovr'] = np.nan\n",
    "\n",
    "    cols_final = [\n",
    "        'sk_crpt', 'sk_data', 'sk_hora',\n",
    "        'rnk_cmc', 'vlr_preco_usd', 'vlr_volume_24h', 'vlr_mktcap', 'vlr_dmn', 'vlr_tovr',\n",
    "        'qtd_pairs', 'qtd_circ_sup', 'qtd_tot_sup', 'vlr_fd_mktcap', 'vlr_mcap_ts',\n",
    "        'pc_ytd', 'pc_1h', 'pc_24h', 'pc_7d', 'pc_30d', 'pc_60d', 'pc_90d'\n",
    "    ]\n",
    "\n",
    "    df_fato_final = df_fato[cols_final].copy()\n",
    "\n",
    "    df_fato_final.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_fato_list = df_fato_final.where(pd.notna(df_fato_final), None).values.tolist()\n",
    "\n",
    "    print(\"Carregando dados na dw.fato_metricas_crpt...\")\n",
    "    with get_connection() as conn, conn.cursor() as cur:\n",
    "        cur.execute(\"TRUNCATE TABLE dw.fato_metricas_crpt RESTART IDENTITY;\")\n",
    "        query = sql.SQL(\"INSERT INTO dw.fato_metricas_crpt ({}) VALUES %s\").format(sql.SQL(', ').join(map(sql.Identifier, cols_final)))\n",
    "        \n",
    "        print(f\"Inserindo {len(df_fato_list)} registros na Fato... (Pode demorar)\")\n",
    "        execute_values(cur, query, df_fato_list, page_size=1000)\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"Fato carregada: {len(df_fato_list)} registros.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar fato: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6608753",
   "metadata": {},
   "source": [
    "## Validação final\n",
    "\n",
    "Esta seção executa uma contagem de linhas em todas as tabelas para verificar o resultado final. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05bcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verificando contagem de linhas final...\")\n",
    "try:\n",
    "    with get_connection() as conn:\n",
    "        query = \"\"\"\n",
    "        SELECT 'dw.dim_crpt' AS Tabela, COUNT(*) AS Total_Linhas FROM dw.dim_crpt\n",
    "        UNION ALL\n",
    "        SELECT 'dw.dim_data' AS Tabela, COUNT(*) AS Total_Linhas FROM dw.dim_data\n",
    "        UNION ALL\n",
    "        SELECT 'dw.dim_hora' AS Tabela, COUNT(*) AS Total_Linhas FROM dw.dim_hora\n",
    "        UNION ALL\n",
    "        SELECT 'dw.fato_metricas_crpt' AS Tabela, COUNT(*) AS Total_Linhas FROM dw.fato_metricas_crpt;\n",
    "        \"\"\"\n",
    "        df_contagem = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        print(\"\\n--- CONTAGEM FINAL DE LINHAS ---\")\n",
    "        display(df_contagem)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao verificar contagens: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd8d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(\"../data_layer/silver/silver_currencies_data.csv\")\n",
    "df.to_sql(\n",
    "    \"fato_metricas_crpt\",\n",
    "    conn,\n",
    "    schema=\"dw\",\n",
    "    if_exists=\"append\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "\n",
    "TENANT = \"SEU_TENANT_ID\"\n",
    "CLIENT = \"SEU_CLIENT_ID\"\n",
    "SECRET = \"SEU_SECRET\"\n",
    "GROUP = \"SEU_WORKSPACE_ID\"\n",
    "\n",
    "token = requests.post(\n",
    "    f\"https://login.microsoftonline.com/{TENANT}/oauth2/v2.0/token\",\n",
    "    data={\n",
    "        \"client_id\": CLIENT,\n",
    "        \"client_secret\": SECRET,\n",
    "        \"scope\": \"https://analysis.windows.net/powerbi/api/.default\",\n",
    "        \"grant_type\": \"client_credentials\"\n",
    "    }\n",
    ").json()[\"access_token\"]\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\"\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# 4) Publicar PBIX no Power BI Online (Linux)\n",
    "# =========================================================\n",
    "\n",
    "PBIX = \"crypto_model.pbix\"  # seu template pbix\n",
    "\n",
    "with open(PBIX, \"rb\") as f:\n",
    "    resp = requests.post(\n",
    "        f\"https://api.powerbi.com/v1.0/myorg/groups/{GROUP}/imports?datasetDisplayName=DW_Crypto\",\n",
    "        headers=headers,\n",
    "        files={\"file\": f}\n",
    "    )\n",
    "\n",
    "import_id = resp.json()[\"id\"]\n",
    "\n",
    "# =========================================================\n",
    "# 5) Obter ID do Dataset importado\n",
    "# =========================================================\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "imports = requests.get(\n",
    "    f\"https://api.powerbi.com/v1.0/myorg/groups/{GROUP}/imports\",\n",
    "    headers=headers\n",
    ").json()\n",
    "\n",
    "DATASET_ID = imports[\"value\"][0][\"datasets\"][0][\"id\"]\n",
    "\n",
    "# =========================================================\n",
    "# 6) Atualizar o dataset publicado\n",
    "# =========================================================\n",
    "\n",
    "requests.post(\n",
    "    f\"https://api.powerbi.com/v1.0/myorg/groups/{GROUP}/datasets/{DATASET_ID}/refreshes\",\n",
    "    headers=headers\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
